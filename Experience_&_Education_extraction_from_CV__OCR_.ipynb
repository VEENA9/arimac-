{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Experience & Education extraction from CV _OCR .ipynb",
      "provenance": [],
      "collapsed_sections": [
        "3Kzfto-ihdPn",
        "ZM78AGPYNi0g",
        "u2zQBkCTKZbM",
        "pUxEN_Pz9-3N",
        "lbnGFOVv9o2i",
        "wa6gdTOp9hWC",
        "yrdr9s3M6dzy",
        "w0kfcgOVcCO-"
      ],
      "toc_visible": true,
      "mount_file_id": "1CPgChuEMca99JOpRS6YYkampOLoHgTlS",
      "authorship_tag": "ABX9TyP7Xl3J3iHmDU1QMzDVjgbN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VEENA9/arimac-/blob/main/Experience_%26_Education_extraction_from_CV__OCR_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Kzfto-ihdPn"
      },
      "source": [
        "#lib"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOigv-x_b3wI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "445f1c69-be7e-4df5-fe63-e283e929f554"
      },
      "source": [
        "!pip3 install Pillow\n",
        "!pip3 install pytesseract\n",
        "!pip3 install pdf2image\n",
        "!sudo apt-get install tesseract-ocr\n",
        "!apt-get install poppler-utils \n",
        "!pip install python-poppler\n",
        "!pip install flask-ngrok"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (7.1.2)\n",
            "Requirement already satisfied: pytesseract in /usr/local/lib/python3.7/dist-packages (0.3.7)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from pytesseract) (7.1.2)\n",
            "Requirement already satisfied: pdf2image in /usr/local/lib/python3.7/dist-packages (1.15.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from pdf2image) (7.1.2)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "tesseract-ocr is already the newest version (4.00~git2288-10f4998a-2).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 34 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "poppler-utils is already the newest version (0.62.0-2ubuntu2.12).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 34 not upgraded.\n",
            "Collecting python-poppler\n",
            "  Using cached https://files.pythonhosted.org/packages/38/77/8f7b5c8ff2c0a7c3afbb3dc8d2342ef2e3ef48053a467e02913e18b73dc6/python-poppler-0.2.2.tar.gz\n",
            "Building wheels for collected packages: python-poppler\n",
            "  Building wheel for python-poppler (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for python-poppler\u001b[0m\n",
            "\u001b[?25h  Running setup.py clean for python-poppler\n",
            "Failed to build python-poppler\n",
            "Installing collected packages: python-poppler\n",
            "    Running setup.py install for python-poppler ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[31mERROR: Command errored out with exit status 1: /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-ddqevl50/python-poppler/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-ddqevl50/python-poppler/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /tmp/pip-record-b6yy46uq/install-record.txt --single-version-externally-managed --compile Check the logs for full command output.\u001b[0m\n",
            "Requirement already satisfied: flask-ngrok in /usr/local/lib/python3.7/dist-packages (0.0.25)\n",
            "Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.7/dist-packages (from flask-ngrok) (1.1.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from flask-ngrok) (2.23.0)\n",
            "Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (1.0.1)\n",
            "Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (7.1.2)\n",
            "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (2.11.3)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (1.1.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (1.24.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->Flask>=0.8->flask-ngrok) (2.0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxaFyq5zQYFU"
      },
      "source": [
        "import os\n",
        "\n",
        "if not os.path.exists('templates'):\n",
        "  os.makedirs('templates')\n",
        "if not os.path.exists('myFold'):\n",
        "  os.makedirs('myFold')  "
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZM78AGPYNi0g"
      },
      "source": [
        "#Filtred DF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOfOnBjBDbTX"
      },
      "source": [
        "# get data from image\n",
        "def save_df(df_img,last_blok):\n",
        "\n",
        "  table_Cv = df_img\n",
        "  # image data to DF\n",
        "  table_Cv_df = pd.DataFrame([x.split('\\t') for x in table_Cv.split('\\n')])\n",
        "  # image data to DF\n",
        "  # add headera same as 1st row\n",
        "  table_Cv_df1 = table_Cv_df.rename(columns=table_Cv_df.iloc[0])\n",
        "  # remove 1st row\n",
        "  table_Cv_df1  = table_Cv_df1[1:]\n",
        "\n",
        "  # filter DF (empty text, left and top 0, remove \"e\":line)\n",
        "  Df_filter_f = table_Cv_df1.loc[(table_Cv_df1.text != ' ') & (table_Cv_df1.left != '0')&(table_Cv_df1.text !='  ')&\n",
        "                              (table_Cv_df1.top != '0')& (table_Cv_df1.text != 'e')& (table_Cv_df1.text != \"\")& \n",
        "                              (table_Cv_df1.conf != \"0\"),['block_num','par_num','width','height',\"conf\",'text']]\n",
        "  \n",
        "  # Df_filter1 = table_Cv_df1.loc[(table_Cv_df1.text.str.len() > 3) & (pd.to_numeric(table_Cv_df1['conf']) > 74)]\n",
        "  # print(Df_filter_f)\n",
        "  # (Df_filter_f.text.str.len() > 3)\n",
        "  Df_f_1 = Df_filter_f[pd.to_numeric(Df_filter_f['conf']) <= 75]\n",
        "  Df_f_2 = Df_filter_f[Df_filter_f.text.str.len() < 3]  \n",
        "  Df_f_3 = Df_f_1.join(Df_f_2, lsuffix='_caller', rsuffix='_other').dropna()\n",
        "  Df_filter1 = Df_filter_f.drop(Df_f_3.index.values.tolist())\n",
        "  Df_filter1 = Df_filter1.rename(columns={'block_num': 'block_n'})\n",
        "  add_no = pd.to_numeric(Df_filter1[\"block_n\"]) +last_blok\n",
        "  Df_filter1['block_num'] = add_no\n",
        "  Df_filter1 = Df_filter1.dropna()\n",
        "  next_first_blok = Df_filter1[\"block_num\"].iloc[0]\n",
        "  # print(Df_filter1) \n",
        "  return Df_filter1,next_first_blok                          "
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uU6WTrVOCQM"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2zQBkCTKZbM"
      },
      "source": [
        "#return head an name"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnI__1_EOsM6"
      },
      "source": [
        "def find_head(dff):\n",
        "  Df_filter = dff  \n",
        "  # Find key heading\n",
        "  key_word = [\"CONTACT\",\"Education\",\"Project\",\"EXPERIENCE\",\"Referees\",\"Skill\",\"About\",\"Award\",\"Professional\",\"INTEREST\",\n",
        "              \"ACTIVITIES\",\"AWARDS\",\"PUBLICATION\",\"Activity\",\"Hobbies\",\"Profile\",\"work\",\"QUALIFICATION\"]\n",
        "  awoid_key = [\"www\",\"@\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"0\",\"=\",\"_\",\",\",\"&\"]\n",
        "  # get guessed heading  \n",
        "  pattern = '|'.join(key_word)\n",
        "  guessed_heading_df =Df_filter[Df_filter['text'].str.contains(pattern,na=False, case=False)]\n",
        "  # get list of guessed head lock num\n",
        "  # print(guessed_heading_df[\"text\"].tolist())\n",
        "  block_num_h_guessd = guessed_heading_df[\"block_num\"].values.tolist()\n",
        "  # print(guessed_heading_df)\n",
        "\n",
        "  # awoid symbols\n",
        "  awoid_pattern = '|'.join(awoid_key)\n",
        "  df_awoid_symbol =Df_filter[Df_filter['text'].str.contains(awoid_pattern,na=False, case=False)] \n",
        "  # print(df_awoid_symbol[\"block_num\"].tolist())\n",
        "  # print(df_awoid_symbol[\"text\"])\n",
        "  df_awoid = Df_filter[~Df_filter['block_num'].isin(df_awoid_symbol[\"block_num\"].values.tolist())]\n",
        "  \n",
        "  # print(df_awoid[\"text\"].tolist())\n",
        "  # get the guessed heading block number\n",
        "    # get original (awoid symbol) block list\n",
        "  block_num_all = df_awoid[\"block_num\"].values.tolist()\n",
        "  # print(block_num_all)\n",
        "  # find raws based same block_num \n",
        "  full_heading_df = df_awoid[df_awoid['block_num'].isin(block_num_h_guessd)]\n",
        "  # print(full_heading_df[\"block_num\"].values.tolist())\n",
        "  # block number from original df\n",
        "  same_block_no=full_heading_df['block_num'].values.tolist()\n",
        "  # print(same_block_no)\n",
        "  # 1 or 2 blocks items (we assume heading must on 1 or 2 words) (get 1 or 2 block word from originl df)\n",
        "  must_head_blok_list = [item for item, count in collections.Counter(same_block_no).items() if (count <= 3)]\n",
        "  # print(must_head_blok_list)\n",
        "  # find df of must head \n",
        "  one_or_two_blok_heading1 = Df_filter[Df_filter['block_num'].isin(must_head_blok_list)]\n",
        "  # print(one_or_two_blok_heading1)\n",
        "\n",
        "  # ///////////// -----|^ find gessed all heading\n",
        "\n",
        "  \n",
        "  # key waigth////// in one_or_two_blok_heading \n",
        "  numaric_w = pd.to_numeric(df_awoid['width'])\n",
        "  numaric_h = pd.to_numeric(df_awoid['height'])\n",
        "  df_waigth = numaric_w * numaric_h\n",
        "  # charector count\n",
        "  D_text_count = df_awoid['text'].str.len()\n",
        "  df_key_waight_all =df_waigth / D_text_count\n",
        "  # print(df_key_waight_all)\n",
        "  # //////////\n",
        "  \n",
        "  \n",
        "\n",
        "  # /////////////// get gussed heading waight and find minimum number of k_waight \n",
        "  numaric_w_h = pd.to_numeric(one_or_two_blok_heading1['width'])\n",
        "  numaric_h_h = pd.to_numeric(one_or_two_blok_heading1['height'])\n",
        "  df_waigth_head = numaric_w_h * numaric_h_h\n",
        "  # print(df_waigth_all.values.tolist())\n",
        "  D_text_count_head = one_or_two_blok_heading1['text'].str.len()\n",
        "  df_key_waight_head = df_waigth_head / D_text_count_head\n",
        "  avg_key_waight = df_key_waight_head.min(skipna=True)\n",
        "  # print(avg_key_waight)\n",
        "  # ///////////////\n",
        "\n",
        "  # print(Df_filter[Df_filter['key waigth'] > 25860 ])\n",
        "  Df_waight_filter = df_key_waight_all[df_key_waight_all >= avg_key_waight] \n",
        "  # print(Df_waight_filter)\n",
        "\n",
        "  Df_waight_filter_text = (pd.concat([Df_waight_filter, df_awoid.reindex(Df_waight_filter.index)], axis=1)).dropna()\n",
        "  # print(Df_waight_filter_text)\n",
        "  \n",
        "  # 1 or 2 or 3 line contain heading : validate\n",
        "  block_num_h1 = Df_waight_filter_text[\"block_num\"].values.tolist()\n",
        "  # print(block_num_h1)\n",
        "\n",
        "  # must_head_blok_list = [item for item, count in collections.Counter(block_num_h1).items() if (count <= 3)]\n",
        "  # print(block_num_h1)\n",
        "  full_heading_df1 = Df_filter[Df_filter['block_num'].isin(block_num_h1)]\n",
        "  fewline_head = full_heading_df1[\"block_num\"].tolist()\n",
        "  # print(fewline_head)\n",
        "  must_head_blok_list = [item for item, count in collections.Counter(fewline_head).items() if (count <= 3)]\n",
        "  df_headind = Df_filter[Df_filter['block_num'].isin(must_head_blok_list)]\n",
        "\n",
        "\n",
        "\n",
        "  # NAME\n",
        "  # //////\n",
        "  df_len_for_name = len(Df_filter) / 3\n",
        "  Df_filter_name = Df_filter.loc[:df_len_for_name]\n",
        "  numaric_w = pd.to_numeric(Df_filter_name['width'])\n",
        "  numaric_h = pd.to_numeric(Df_filter_name['height'])\n",
        "  df_waigth = numaric_w * numaric_h\n",
        "  # charector count\n",
        "  D_text_count = Df_filter['text'].str.len()\n",
        "  df_key_waight_all =df_waigth / D_text_count\n",
        "  # //////\n",
        "  Df_waight_filter = df_key_waight_all[df_key_waight_all >= avg_key_waight] \n",
        "  # print(Df_waight_filter)\n",
        "  \n",
        "  Df_waight_filter_text = (pd.concat([Df_waight_filter, Df_filter_name], axis=1)).dropna()\n",
        "  \n",
        "  pattern = '|'.join(key_word)\n",
        "  guessed_heading_df =Df_waight_filter_text[~Df_waight_filter_text['text'].str.contains(pattern,na=False, case=False)]\n",
        "  awoid_pattern = '|'.join(awoid_key)\n",
        "  df_awoid_symbol =guessed_heading_df[~guessed_heading_df['text'].str.contains(awoid_pattern,na=False, case=False)]\n",
        "  fewline_head_name = df_awoid_symbol[\"block_num\"].tolist()\n",
        "  # print(df_awoid_symbol)\n",
        "  \n",
        "  # print(must_head_blok_list1)\n",
        "  must_head_blok_list2 = Df_filter_name[Df_filter_name['block_num'].isin(fewline_head_name)]\n",
        "  must_head_blok_list1 = [item for item, count in collections.Counter(must_head_blok_list2[\"block_num\"].tolist()).items() if (count <= 7)]\n",
        "  # print(must_head_blok_list1)\n",
        "  must_head_blok_list = Df_filter_name[Df_filter_name['block_num'].isin(must_head_blok_list1)]\n",
        "  # must_head_blok_list = (pd.merge_asof(df2, Df_filter_name,on=index))\n",
        "  # print(must_head_blok_list)\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "# /////// big word find\n",
        "  numaric_w_n = pd.to_numeric(must_head_blok_list['width'])\n",
        "  numaric_h_n = pd.to_numeric(must_head_blok_list['height'])\n",
        "  df_waigth_n = numaric_w_n * numaric_h_n\n",
        "  # charector count\n",
        "  D_text_count = must_head_blok_list['text'].str.len()\n",
        "  df_key_waight_all = df_waigth_n / D_text_count\n",
        "  # print(df_key_waight_all)\n",
        "\n",
        "  must_head_blok_list3 = must_head_blok_list.loc[df_key_waight_all.index.tolist()]\n",
        "  # print(must_head_blok_list3)\n",
        "  pattern = '|'.join(key_word)\n",
        "  must_head_blok_list =must_head_blok_list3[~must_head_blok_list3['text'].str.contains(pattern,na=False, case=False)]\n",
        "  # print(must_head_blok_list)\n",
        "  df_key_waight_all1 = df_key_waight_all.loc[must_head_blok_list.index.values.tolist()]\n",
        "  name_index = df_key_waight_all1.idxmax()  \n",
        "  # print(df_key_waight_all1)\n",
        "  block_num = must_head_blok_list.loc[name_index].block_num\n",
        "  # print(block_num)\n",
        "  # find raws based same block_num \n",
        "  df2 =must_head_blok_list.loc[must_head_blok_list[\"block_num\"] == block_num]\n",
        "  # print(df2)\n",
        "  # get text of same raw and mearg as a string.\n",
        "  ln = df2['text'].values\n",
        "  name= ' '.join(ln)\n",
        "  \n",
        "  # 1 or 2 or 3 line contain heading : validate\n",
        "  # block_num_h1 = Df_waight_filter_text[\"block_num\"].values.tolist()\n",
        "  # pattern = '|'.join(key_word)\n",
        "  # df_headind_n =df_headind[~df_headind['text'].str.contains(pattern,na=False, case=False)]  \n",
        "\n",
        "  return df_headind, name"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUxEN_Pz9-3N"
      },
      "source": [
        "#find head (heading, no heading)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IglUh0nwato6"
      },
      "source": [
        "def details(Df_filter,Df_head,word):\n",
        "  # word = \"EXPERIENCE\"\n",
        "  head_index = Df_head[Df_head['text'].str.contains(word, na=False, case=False)]\n",
        "  # print(head_index)\n",
        "  if head_index.empty == True:\n",
        "    print(word,\" is not in the Heading list.\")\n",
        "    word_set = no_heading(Df_filter,Df_head,word)\n",
        "  else:\n",
        "    print(word, \"is in the Heading list.\")\n",
        "    word_set = heading(Df_filter,Df_head,word,head_index)\n",
        "\n",
        "  return word_set"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbnGFOVv9o2i"
      },
      "source": [
        "# heading = TRUE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VukqALZKRZD"
      },
      "source": [
        "# heading = TRUE\n",
        "def heading(Df_filter,Df_head,word,head_index):\n",
        "    \n",
        "  # get Details(df) between two heading\n",
        "  start_index = head_index.index.values[0] \n",
        "  df_head_index_list = Df_head.index.values.tolist()\n",
        "  end_index = df_head_index_list[(df_head_index_list.index(start_index)) % len(df_head_index_list)] \n",
        "\n",
        "  if end_index==start_index:\n",
        "    print(word,\" is last heading\")\n",
        "      # end_index = None\n",
        "      # end_index = int()\n",
        "\n",
        "    df_data_word = Df_filter.loc[start_index+1 : ]\n",
        "    \n",
        "    # get number of block count , between two heading(ovvoru blocklayum periya k_waightku uriya word.a eduthu, athu Key_wordku\n",
        "    # samana irukkanu paarthu = enda athodaye stop pannanum, illana adutha head vara vaasikanum)\n",
        "    # get block numbers\n",
        "    blok_tuple = (df_data_word[\"block_num\"].drop_duplicates())\n",
        "    # print(blok_tuple)\n",
        "    blok_len = len(blok_tuple)\n",
        "    # print(blok_len)\n",
        "    b = 0\n",
        "    while (b < blok_len):\n",
        "      blok_num=blok_tuple.iloc[b]\n",
        "      a_blok_text_set = df_data_word[df_data_word[\"block_num\"].isin([blok_num])]\n",
        "      numaric_w_w = pd.to_numeric(a_blok_text_set['width'])\n",
        "      numaric_h_w = pd.to_numeric(a_blok_text_set['height'])\n",
        "      df_waigth_word = numaric_w_w * numaric_h_w\n",
        "      D_text_count_head = a_blok_text_set['text'].str.len()\n",
        "      df_key_waight_word = df_waigth_word / D_text_count_head\n",
        "      max_key_waight = df_key_waight_word.idxmax()\n",
        "      max_w_word_a_blok = df_data_word[\"text\"].loc[[max_key_waight]]\n",
        "      # print(max_w_word_a_blok)\n",
        "      \n",
        "      key_word = [\"Education\",\"Project\",\"Referees\",\"Skill\",\"Award\",\"Professional\",\"INTEREST\",\n",
        "                  \"ACTIVITIES\",\"PUBLICATION\",\"Activity\",\"Hobbies\"]\n",
        "\n",
        "      pattern = '|'.join(key_word)\n",
        "      df_if_in_word = max_w_word_a_blok.str.contains(pattern,na=False, case=False).values[0]\n",
        "      # print(max_w_word_a_blok.str.contains(pattern,na=False, case=False))\n",
        "      if df_if_in_word == True:\n",
        "        end_index1 = max_w_word_a_blok.index.values[0] \n",
        "        df_data_word_finl = Df_filter.loc[start_index+1 : end_index1-1]\n",
        "        break \n",
        "      else:\n",
        "        df_data_word_finl = Df_filter.loc[start_index+1 :]   \n",
        "      b += 1\n",
        "\n",
        "  else:\n",
        "    print(word,\" is center heading\")\n",
        "    df_data_word = Df_filter.loc[start_index+1 : end_index-1 ]\n",
        "    \n",
        "    \n",
        "    # get number of block count , between two heading(ovvoru blocklayum periya k_waightku uriya word.a eduthu, athu Key_wordku\n",
        "    # samana irukkanu paarthu = enda athodaye stop pannanum, illana adutha head vara vaasikanum)\n",
        "    # get block numbers\n",
        "    blok_tuple = (df_data_word[\"block_num\"].drop_duplicates())\n",
        "    # print(blok_tuple)\n",
        "    blok_len = len(blok_tuple)\n",
        "    # print(blok_len)\n",
        "    b = 0\n",
        "    while (b < blok_len):\n",
        "      blok_num=blok_tuple.iloc[b]\n",
        "      a_blok_text_set = df_data_word[df_data_word[\"block_num\"].isin([blok_num])]\n",
        "      numaric_w_w = pd.to_numeric(a_blok_text_set['width'])\n",
        "      numaric_h_w = pd.to_numeric(a_blok_text_set['height'])\n",
        "      df_waigth_word = numaric_w_w * numaric_h_w\n",
        "      D_text_count_head = a_blok_text_set['text'].str.len()\n",
        "      df_key_waight_word = df_waigth_word / D_text_count_head\n",
        "      max_key_waight = df_key_waight_word.idxmax()\n",
        "      max_w_word_a_blok = df_data_word[\"text\"].loc[[max_key_waight]]\n",
        "      \n",
        "      key_word = [\"Education\",\"Project\",\"Referees\",\"Skill\",\"Award\",\"Professional\",\"INTEREST\",\n",
        "                  \"ACTIVITIES\",\"PUBLICATION\",\"Activity\",\"Hobbies\"]\n",
        "\n",
        "      pattern = '|'.join(key_word)\n",
        "      df_if_in_word = max_w_word_a_blok.str.contains(pattern,na=False, case=False).values[0]\n",
        "     \n",
        "      if df_if_in_word == True:\n",
        "        end_index1 = max_w_word_a_blok.index.values[0] \n",
        "        df_data_word_finl = Df_filter.loc[start_index+1 : end_index1-1]\n",
        "        break \n",
        "      else:\n",
        "        df_data_word_finl = Df_filter.loc[start_index+1 :end_index-1]   \n",
        "      b += 1\n",
        "\n",
        "\n",
        "  # df_data_word_finl = Df_filter.loc[start_index+1 : end_index]\n",
        "  ln = df_data_word_finl['text'].values\n",
        "  details_text = '\\n'.join(' '.join(ln).split('. '))\n",
        "  print(details_text)\n",
        "  return details_text"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wa6gdTOp9hWC"
      },
      "source": [
        "# Heading Fales"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SP7T-yr9QHz0"
      },
      "source": [
        "# Heading Fales \n",
        "# word.a thedi eduthu, athila periya k_waight irukratha select panni\n",
        "# athukku aduthatha ulla head.a thedi eduththu, idaila ullatha df aakkanum, next ,Heading True.la ulla pola seiyanum.\n",
        "\n",
        "def no_heading(Df_filter,Df_head,word):\n",
        "\n",
        "  head_index_tuple = Df_filter[Df_filter['text'].str.contains(word, na=False, case=False)]\n",
        "  # print(head_index_tuple)\n",
        "\n",
        "  if head_index_tuple.empty:\n",
        "    print(word,\" is not in the cv\")\n",
        "    details_text = word+\" is not in the cv\"    \n",
        "\n",
        "  else:\n",
        "    numaric_w_w = pd.to_numeric(head_index_tuple['width'])\n",
        "    numaric_h_w = pd.to_numeric(head_index_tuple['height'])\n",
        "    df_waigth_word = numaric_w_w * numaric_h_w\n",
        "    D_text_count_head = head_index_tuple['text'].str.len()\n",
        "    df_key_waight_word = df_waigth_word / D_text_count_head\n",
        "    # print(df_key_waight_word)\n",
        "    max_key_waight = df_key_waight_word.idxmax()\n",
        "    # print(max_key_waight)\n",
        "    max_w_word_a_blok = Df_filter[\"text\"].loc[[max_key_waight]]\n",
        "\n",
        "    big_word_index = max_w_word_a_blok.index[0]\n",
        "    start_index = big_word_index\n",
        "    # find next head in Df_head\n",
        "      # find last head index\n",
        "    # print(Df_head[\"text\"].tolist())\n",
        "    head_index = Df_head.index.tolist() \n",
        "    last_head_index = Df_head.iloc[[-1]].index[0]\n",
        "    # print(last_head_index , big_word_index )\n",
        "    \n",
        "    # big word last head or belove to head\n",
        "    if last_head_index <= big_word_index:\n",
        "      df_data_word_bigword_l = Df_filter.loc[big_word_index+1 :]  \n",
        "      df_data_word_finl = Df_filter.loc[start_index+1 :]\n",
        "      print(\"word in last heading\")\n",
        "\n",
        "       # get number of block count , between two heading(ovvoru blocklayum periya k_waightku uriya word.a eduthu, athu Key_wordku\n",
        "      # samana irukkanu paarthu = enda athodaye stop pannanum, illana adutha head vara vaasikanum)\n",
        "      # get block numbers\n",
        "      blok_tuple = (df_data_word_bigword[\"block_num\"].drop_duplicates())\n",
        "      # print(df_data_word[df_data_word[\"block_num\"].isin([17])])\n",
        "      blok_len = len(blok_tuple)\n",
        "      # print(blok_len)\n",
        "      b = 0\n",
        "      while (b < blok_len):\n",
        "        blok_num=blok_tuple.iloc[b]\n",
        "        a_blok_text_set = df_data_word_bigword[df_data_word_bigword[\"block_num\"].isin([blok_num])]\n",
        "        numaric_w_w = pd.to_numeric(a_blok_text_set['width'])\n",
        "        numaric_h_w = pd.to_numeric(a_blok_text_set['height'])\n",
        "        df_waigth_word = numaric_w_w * numaric_h_w\n",
        "        D_text_count_head = a_blok_text_set['text'].str.len()\n",
        "        df_key_waight_word = df_waigth_word / D_text_count_head\n",
        "        max_key_waight = df_key_waight_word.idxmax()\n",
        "        max_w_word_a_blok = df_data_word_bigword[\"text\"].loc[[max_key_waight]]\n",
        "        \n",
        "        key_word = [\"Education\",\"Project\",\"Referees\",\"Skill\",\"Award\",\"Professional\",\"INTEREST\",\n",
        "                    \"ACTIVITIES\",\"PUBLICATION\",\"Activity\",\"Hobbies\"]\n",
        "\n",
        "        pattern = '|'.join(key_word)\n",
        "        df_if_in_word = max_w_word_a_blok.str.contains(pattern,na=False, case=False).values[0]\n",
        "        # print(df_if_in_word)\n",
        "        if df_if_in_word == True:\n",
        "          end_index1 = max_w_word_a_blok.index.values[0]\n",
        "          df_data_word_finl = Df_filter.loc[start_index+1 : end_index1-1]           \n",
        "          break  \n",
        "        else:\n",
        "          df_data_word_finl = Df_filter.loc[start_index+1 :]  \n",
        "        b +=1       \n",
        "        \n",
        "\n",
        "    else: \n",
        "      print(\"word in center area\")\n",
        "      next_head_bigword = [i for i in head_index if i > big_word_index][0]\n",
        "      # print(head_index)\n",
        "      # print(big_word_index+1 , next_head_bigword-1 )\n",
        "      df_data_word_bigword = Df_filter.loc[big_word_index+1 : next_head_bigword-1]\n",
        "      # print(df_data_word_bigword)\n",
        "\n",
        "      # get number of block count , between two heading(ovvoru blocklayum periya k_waightku uriya word.a eduthu, athu Key_wordku\n",
        "      # samana irukkanu paarthu = enda athodaye stop pannanum, illana adutha head vara vaasikanum)\n",
        "      # get block numbers\n",
        "      blok_tuple = (df_data_word_bigword[\"block_num\"].drop_duplicates())\n",
        "      # print(df_data_word[df_data_word[\"block_num\"].isin([17])])\n",
        "      blok_len = len(blok_tuple)\n",
        "      # print(blok_len)\n",
        "      b = 0\n",
        "      while (b < blok_len):\n",
        "        blok_num=blok_tuple.iloc[b]\n",
        "        a_blok_text_set = df_data_word_bigword[df_data_word_bigword[\"block_num\"].isin([blok_num])]\n",
        "        numaric_w_w = pd.to_numeric(a_blok_text_set['width'])\n",
        "        numaric_h_w = pd.to_numeric(a_blok_text_set['height'])\n",
        "        df_waigth_word = numaric_w_w * numaric_h_w\n",
        "        D_text_count_head = a_blok_text_set['text'].str.len()\n",
        "        df_key_waight_word = df_waigth_word / D_text_count_head\n",
        "        max_key_waight = df_key_waight_word.idxmax()\n",
        "        max_w_word_a_blok = df_data_word_bigword[\"text\"].loc[[max_key_waight]]\n",
        "        \n",
        "        key_word = [\"Education\",\"Project\",\"Referees\",\"Skill\",\"Award\",\"Professional\",\"INTEREST\",\n",
        "                    \"ACTIVITIES\",\"PUBLICATION\",\"Activity\",\"Hobbies\"]\n",
        "\n",
        "        pattern = '|'.join(key_word)\n",
        "        df_if_in_word = max_w_word_a_blok.str.contains(pattern,na=False, case=False).values[0]\n",
        "        # print(df_if_in_word)\n",
        "        if df_if_in_word == True:\n",
        "          end_index1 = max_w_word_a_blok.index.values[0] \n",
        "          df_data_word_finl = Df_filter.loc[start_index+1 : end_index1-1]\n",
        "          break \n",
        "        else:\n",
        "          end_index = next_head_bigword-1 \n",
        "          df_data_word_finl = Df_filter.loc[start_index+1 : end_index]  \n",
        "        b +=1\n",
        "        \n",
        "    # df_data_word_finl = Df_filter.loc[start_index+1 : end_index]\n",
        "    ln = df_data_word_finl['text'].values\n",
        "    details_text = '\\n'.join(' '.join(ln).split('. '))\n",
        "    # print(details_text) \n",
        "    return details_text"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrdr9s3M6dzy"
      },
      "source": [
        "#Front - html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61BCTL9A6cjO"
      },
      "source": [
        "from flask_ngrok import run_with_ngrok\n",
        "from flask import Flask, render_template\n",
        "text = '''\n",
        "<!DOCTYPE html>\n",
        "<html>\n",
        "<head>\n",
        "  <meta charset=\"UTF-8\">\n",
        "  <title>Upload your Cv as a Pdf</title>\n",
        "</head>\n",
        "\n",
        "<body>\n",
        "\n",
        "    <input id=\"fileupload\" type=\"file\" name=\"fileupload\" />\n",
        "    <button id=\"upload-button\" onclick=\"uploadFile()\"> Upload </button>\n",
        "    <h5>NAME</h5>\n",
        "    <P id = \"NAME\"></p>\n",
        "    <br/>\n",
        "    <h5>EXPERIENCE</h5>\n",
        "    <P id = \"EXPERIENCE\"></p>\n",
        "    <br/>\n",
        "    <h5>EDUCATION</h5>\n",
        "    <P id = \"Education\"></p>\n",
        "\n",
        "<script>\n",
        "async function uploadFile() {\n",
        "    let formData = new FormData();           \n",
        "    formData.append(\"file\", fileupload.files[0]);\n",
        "    let response = await fetch('/second', \n",
        "    { method: \"POST\", body: formData});    \n",
        "     \n",
        "    let data = await response.json()\n",
        "    document.getElementById(\"NAME\").innerHTML = data.username;\n",
        "    document.getElementById(\"EXPERIENCE\").innerHTML = data.EXPERIENCE; \n",
        "    document.getElementById(\"Education\").innerHTML = data.Education;\n",
        "    console.log(response)\n",
        "    console.log(data)\n",
        "    return fileupload.files[0];\n",
        "    \n",
        "}\n",
        "</script>\n",
        "  \n",
        "\n",
        "</body>\n",
        "</html>\n",
        "'''\n",
        "file = open(\"templates/text.html\",\"w\")\n",
        "file.write(text)\n",
        "file.close()"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0kfcgOVcCO-"
      },
      "source": [
        "#DF by OCR "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9TWITHc7jnY"
      },
      "source": [
        "import json\n",
        "import collections\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pdf2image\n",
        "try:\n",
        "    from PIL import Image\n",
        "except ImportError:\n",
        "    import Image\n",
        "import pytesseract\n",
        "\n",
        "\n",
        "def pdf_to_img(pdf_file):\n",
        "    return pdf2image.convert_from_path(pdf_file)\n",
        "\n",
        "\n",
        "def ocr_core(file):\n",
        "    df_text = pytesseract.image_to_data(file)\n",
        "    return df_text\n",
        "\n",
        "def head_find(Df_filter1):\n",
        "    dff1 = Df_filter1\n",
        "    Df_head_e = pd.DataFrame([]) \n",
        "    df_head1,name = find_head(dff1) \n",
        "    Df_head_e = Df_head_e.append(df_head1)\n",
        "    return Df_head_e,name\n",
        "\n",
        "\n",
        "def print_pages(pdf_file):\n",
        "    images = pdf_to_img(pdf_file)\n",
        "    Df_filter_e = pd.DataFrame([])\n",
        "    # Df_head = pd.DataFrame([])\n",
        "    for pg, img in enumerate(images):\n",
        "        df_img = ocr_core(img)\n",
        "        # dff = save_df(df_img)\n",
        "        if Df_filter_e.empty == True:\n",
        "          df_fill1,next_first_blok=save_df(df_img,0) \n",
        "          Df_filter_e = Df_filter_e.append(df_fill1,ignore_index=True)\n",
        "        else:\n",
        "          last_blok = Df_filter_e[\"block_num\"].iloc[-1]+1\n",
        "          blok_add_no = last_blok - next_first_blok\n",
        "          df_fill1,next_first_blok=save_df(df_img,blok_add_no) \n",
        "          Df_filter_e = Df_filter_e.append(df_fill1,ignore_index=True)\n",
        "        \n",
        "    \n",
        "    Df_head,name = head_find(Df_filter_e)\n",
        "\n",
        "    word_set = []\n",
        "    word = [\"EXPERIENCE\",\"Education\"]\n",
        "    w=0\n",
        "    while (w < len(word)):\n",
        "      D_words = details(Df_filter_e,Df_head,word[w])\n",
        "      word_set.append(D_words)\n",
        "      w += 1\n",
        "    a_series = pd.Series(word_set, word)  \n",
        "    result = a_series.to_json(orient=\"index\")\n",
        "    parsed = json.loads(result)\n",
        "    word_json = json.dumps(parsed, indent=4)\n",
        "    # print(word_json)\n",
        "    x = {\"username\": name }\n",
        "    j_name = json.dumps(x)\n",
        "    j_name = json.loads(j_name)\n",
        "    word_json = json.loads(word_json)\n",
        "    # print(\"name\",j_name)\n",
        "    word_json.update(j_name)\n",
        "    # print(word_json)\n",
        "    return word_json\n",
        "    # print(Df_head)\n",
        "    # print(Df_filter_e)\n",
        "\n",
        "    # Df_head.to_csv (r'/content/head.csv', index = False, header=True)\n",
        "    # Df_filter_e.to_csv (r'/content/data1.csv', index = False, header=True) \n",
        "       \n",
        "# print_pages('sample.pdf')"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdeQCBzx7LmO"
      },
      "source": [
        "#main - front/json"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qV--d5-w7dMP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87cb57dd-b925-484d-c1f0-81966dda4d0d"
      },
      "source": [
        "from flask_ngrok import run_with_ngrok \n",
        "from flask import Flask, render_template, request, redirect, url_for, render_template, send_from_directory\n",
        "from werkzeug.utils import secure_filename\n",
        "from flask import request, jsonify\n",
        "app = Flask(__name__)\n",
        "run_with_ngrok(app)\n",
        "@app.route('/')\n",
        "def text():\n",
        "  return render_template('text.html')\n",
        "\n",
        "@app.route('/second', methods = ['GET', 'POST'])\n",
        "def get_details():\n",
        "  if request.method == 'POST':\n",
        "        f = request.files['file']\n",
        "        name = f.save(f.filename)\n",
        "        file_path = '/content/'+f.filename\n",
        "        result= print_pages(file_path) \n",
        "        \n",
        "        return  result\n",
        "\n",
        "     \n",
        "\t\t\n",
        "if __name__ == '__main__':\n",
        "  \n",
        "   app.run()\n",
        "   "
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " * Serving Flask app \"__main__\" (lazy loading)\n",
            " * Environment: production\n",
            "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
            "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
            " * Debug mode: off\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " * Running on http://304ed557e976.ngrok.io\n",
            " * Traffic stats available on http://127.0.0.1:4040\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [30/May/2021 15:20:48] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [30/May/2021 15:20:49] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "127.0.0.1 - - [30/May/2021 15:20:50] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [30/May/2021 15:21:24] \"\u001b[37mPOST /second HTTP/1.1\u001b[0m\" 200 -\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "EXPERIENCE is in the Heading list.\n",
            "EXPERIENCE  is last heading\n",
            "TRAINEE SOFTWARE ENGINEER - ZILLIONE BUSINESS SOLUTIONS (PVT) LTD (August 2018 — February 2019)\n",
            "Act as a full-stack developer on a transaction-based project named Cashbook\n",
            "Tools and Technologies: ASP.NET, C#, Angular5 HTML/CSS, Visual Studio, Visual Studio Code AWARDS AND ACHIEVEMENTS Australian National Chemistry Quiz\n",
            "Awarded with High Distinction in 2009 and 2011\n",
            "Human Rights Quiz Competition 2010\n",
            "Conducted by Ministry of Education of Eastern Province and the National Peace Council of Sri Lanka\n",
            "Certificate in English Language Examination, Eastern University of Sri Lanka\n",
            "Awarded with Distinction in Junior and Intermediate Level.\n",
            "Education is in the Heading list.\n",
            "Education  is last heading\n",
            "HIGHER EDUCATION BSc (Hons) in Information Technology\n",
            "Faculty of Information Technology, University of Moratuwa\n",
            "SECONDARY EDUCATION G.C.E Advanced Level (Biological Science Stream) St Cecilia’s Girls’ National School Batticaloa\n",
            "Results : Chemistry — B , Physics B , Biology — A,General English -A\n",
            "Z Score : 1.601 G.C.E Ordinary Level St Cecilia’s Girls’ National School Batticaloa\n",
            "Results : 8A’s, B (English Medium) WORK EXPERIENCE TRAINEE SOFTWARE ENGINEER - ZILLIONE BUSINESS SOLUTIONS (PVT) LTD (August 2018 — February 2019)\n",
            "Act as a full-stack developer on a transaction-based project named Cashbook\n",
            "Tools and Technologies: ASP.NET, C#, Angular5 HTML/CSS, Visual Studio, Visual Studio Code AWARDS AND ACHIEVEMENTS Australian National Chemistry Quiz\n",
            "Awarded with High Distinction in 2009 and 2011\n",
            "Human Rights Quiz Competition 2010\n",
            "Conducted by Ministry of Education of Eastern Province and the National Peace Council of Sri Lanka\n",
            "Certificate in English Language Examination, Eastern University of Sri Lanka\n",
            "Awarded with Distinction in Junior and Intermediate Level.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}